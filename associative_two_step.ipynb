{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fit two step task with an associative algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy.stats\n",
    "import patsy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import basinhopping\n",
    "from sklearn import linear_model\n",
    "import multiprocessing\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = .4 #learning rate\n",
    "m=3.0 #temperature (not inverse)\n",
    "p=.4 #tendency to repeat actions >1 means perseveration (from Daw 2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Helper RL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set up data structures and initialize reward and transition probabilities\n",
    "def initialize():\n",
    "    #initialize data structures\n",
    "    objects = ['a1','a2','b1','b2','c1','c2']\n",
    "    states = ['a','b','c','terminal']\n",
    "    actions = ['1','2']\n",
    "\n",
    "    #Initialize transition, rewards, values matrics\n",
    "    transitions = {}\n",
    "    rewards = {}\n",
    "    V = {}\n",
    "    associations = {}\n",
    "    for s in states:\n",
    "        transitions[s] = {}\n",
    "        rewards[s] = {}\n",
    "        for a in actions:\n",
    "            transitions[s][a] = {}\n",
    "            rewards[s][a] = 0\n",
    "    for o in objects:\n",
    "        V[o] = 0\n",
    "        associations[o] = {}\n",
    "\n",
    "    for o1 in associations:\n",
    "        for o2 in objects:\n",
    "            if o1 != o2: #avoid self associations\n",
    "                associations[o1][o2] = 0\n",
    "\n",
    "    #fill in transition probs\n",
    "    for s1 in states:\n",
    "        for a in actions:\n",
    "            for s2 in states:\n",
    "                transitions[s1][a][s2] = 0         \n",
    "    transitions['b']['1']['terminal'] = 1\n",
    "    transitions['b']['2']['terminal'] = 1\n",
    "    transitions['c']['1']['terminal'] = 1\n",
    "    transitions['c']['2']['terminal'] = 1\n",
    "    transitions['a']['1']['b'] = .7\n",
    "    transitions['a']['1']['c'] = .3\n",
    "    transitions['a']['2']['b'] = .3\n",
    "    transitions['a']['2']['c'] = .7\n",
    "\n",
    "    #set up reward probs\n",
    "    rewards['b']['1'] = .7\n",
    "    rewards['b']['2'] = .3\n",
    "    rewards['c']['1'] = .3\n",
    "    rewards['c']['2'] = .7\n",
    "    \n",
    "    return transitions, rewards, V, associations, objects, states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gradually shift reward probabilitites to encourage learning\n",
    "def update_rewards(rewards):\n",
    "    for s in ['b','c']: #only update end states\n",
    "        for a in actions:\n",
    "            shift = np.random.normal(0,.025)\n",
    "            if (rewards[s][a] + shift > .75) or (rewards[s][a] + shift < .25): #reflecting boundaries\n",
    "                rewards[s][a] = rewards[s][a] - shift\n",
    "            else:\n",
    "                rewards[s][a] = rewards[s][a] + shift\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#draw a reward according to reward probability functions\n",
    "def get_reward(state,action,rewards):\n",
    "    return scipy.stats.bernoulli.rvs(rewards[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get new state according to state, action, and transition probabilities\n",
    "def next_state(state,action): \n",
    "    probs = map(lambda x: transitions[state][action][x], states)\n",
    "    return np.random.choice(a=states,p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pick an action according to softmax\n",
    "def get_action(state,V,last_a_action):\n",
    "    Vs = map(lambda a: V[state+a],actions) #get values of each object in state\n",
    "    if state == 'a': #model perseveration\n",
    "        if last_a_action == '1':\n",
    "            Vs[0] = Vs[0] + p\n",
    "        else:\n",
    "            Vs[1] = Vs[1] + p\n",
    "    normalizing_constant = np.sum(map(lambda v: np.exp(m*v),Vs)) #get total value of state\n",
    "    probs = map(lambda v: np.exp(v*m), Vs)\n",
    "    probs = probs / normalizing_constant\n",
    "    return np.random.choice(a=actions,p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#every time a transition occurs, increment the association between those two objects\n",
    "def update_associations(state,new_state,action,associations, nsteps):\n",
    "    if new_state != 'terminal':\n",
    "        nsteps +=1\n",
    "        for a in actions:\n",
    "            associations[state + action][new_state + a] = associations[state + action][new_state + a] + 1\n",
    "            associations[new_state + a][state + action] = associations[state + action][new_state + a] #make symmetric\n",
    "    return associations, nsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_value(rew,state,new_state,action,V,associations,nsteps):\n",
    "#Not sure whether to have value updated between first and second-stage choices. Doesn't seem to influence results one way or another\n",
    "#     if new_state != 'terminal':\n",
    "#         delta = rew + max(V[new_state + actions[0]],V[new_state + actions[1]]) - V[state+action]\n",
    "#     else:\n",
    "#         delta = rew - V[state+action]\n",
    "    delta = rew - V[state+action]\n",
    "    V[state+action] = V[state+action] + alpha*delta\n",
    "    \n",
    "    #percolate value one step back, weighted by the strength of association\n",
    "    for o in associations[state+action]:\n",
    "        delta = rew - V[o]\n",
    "        V[o] = V[o] + associations[state+action][o] * alpha* delta *4/ nsteps #4/nsteps is to normalize \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Runs through the MDP until terminal state, keeping track of the output\n",
    "def take_step(state,rewards,associations,V,nsteps,output,last_a_action):\n",
    "    if state == 'terminal': #end state\n",
    "        rewards = update_rewards(rewards)\n",
    "        return rewards, associations, V, nsteps \n",
    "\n",
    "    #do standard MDP stuff\n",
    "    action = get_action(state,V,last_a_action)\n",
    "    new_state = next_state(state,action)\n",
    "    rew = get_reward(state,action,rewards)\n",
    "    if state == 'a':\n",
    "        last_a_action = action\n",
    "        \n",
    "    #log what's happening\n",
    "    output['rew'].append(rew)\n",
    "    output['action'].append(action)\n",
    "    output['newstate'].append(new_state)\n",
    "    output['state'].append(state)\n",
    "    \n",
    "    #update values and associations\n",
    "    associations,nsteps = update_associations(state,new_state,action,associations,nsteps) #update associations\n",
    "    value = update_value(rew,state,new_state,action,V,associations,nsteps)\n",
    "    \n",
    "    return take_step(new_state,rewards,associations,V,nsteps,output,last_a_action) #recurse until terminal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Main simulation code\n",
    "ntrials = 5000\n",
    "transitions, rewards, V, associations, objects, states, actions = initialize()\n",
    "nsteps = 0.0\n",
    "last_a_action = '1' #need one to get started\n",
    "output = {'state':[],'action':[],'newstate':[],'rew':[]}\n",
    "for i in range(ntrials):\n",
    "    rewards, associations, V, nsteps = take_step('a',rewards,associations,V,nsteps,output,last_a_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Analyze output by transition type and stay/shift behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#analyze wherer transition was common or rare\n",
    "output['transition_type'] = []\n",
    "for n,s in enumerate(output['newstate']):\n",
    "    if s  == 'terminal':\n",
    "        output['transition_type'].append('end')\n",
    "    elif (s == 'b' and output['action'][n] == '1') or (s == 'c' and output['action'][n] == '2'):\n",
    "        output['transition_type'].append('common')\n",
    "    elif (s == 'b' and output['action'][n] == '2') or (s == 'c' and output['action'][n] == '1'):\n",
    "        output['transition_type'].append('rare')\n",
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate whether a first level action was stay or switch stay and switch\n",
    "output['stay'] = np.nan\n",
    "a_indices =  output[output['state'] == 'a'].index\n",
    "a_indices = a_indices.values\n",
    "stay_or_switch = ['np.nan']\n",
    "for n,idx in enumerate(a_indices):\n",
    "    if n>0:\n",
    "        last_action = output.iloc[a_indices[n-1]].action\n",
    "        current_action = output.iloc[a_indices[n]].action\n",
    "        if last_action == current_action:\n",
    "            stay_or_switch.append('stay')\n",
    "        else:\n",
    "            stay_or_switch.append('switch')\n",
    "output.ix[output['state'] == 'a','stay']  = stay_or_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#parse results\n",
    "results = {'rewarded':{'common':[],'rare':[]},'nonrewarded':{'common':[],'rare':[]}}\n",
    "for r in ['rewarded','nonrewarded']:\n",
    "    if r == 'rewarded':\n",
    "        rew = 1\n",
    "    else:\n",
    "        rew = 0\n",
    "        \n",
    "    indices = output[(output['newstate']=='terminal') & (output['rew'] == rew)].index[:-1] #indices of terminal state\n",
    "    transition_type = output.iloc[indices-1]['transition_type'].values #common or rare\n",
    "    action = output.iloc[indices+1]['stay'].values #stay or switch\n",
    "    \n",
    "    \n",
    "    for c in ['common','rare']:\n",
    "        choices = list(action[transition_type == c])\n",
    "        results[r][c] = choices.count('stay')/float(len(choices))\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        nonrewarded  rewarded\n",
      "common     0.516351  0.758699\n",
      "rare       0.616231  0.597419\n"
     ]
    }
   ],
   "source": [
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 a2 0.0\n",
      "a1 b1 0.5326\n",
      "a1 b2 0.5326\n",
      "a1 c2 0.226\n",
      "a1 c1 0.226\n",
      "a2 a1 0.0\n",
      "a2 b1 0.0746\n",
      "a2 b2 0.0746\n",
      "a2 c2 0.1668\n",
      "a2 c1 0.1668\n",
      "b1 a1 0.5326\n",
      "b1 a2 0.0746\n",
      "b1 b2 0.0\n",
      "b1 c2 0.0\n",
      "b1 c1 0.0\n",
      "b2 a1 0.5326\n",
      "b2 a2 0.0746\n",
      "b2 b1 0.0\n",
      "b2 c2 0.0\n",
      "b2 c1 0.0\n",
      "c2 a1 0.226\n",
      "c2 a2 0.1668\n",
      "c2 b1 0.0\n",
      "c2 b2 0.0\n",
      "c2 c1 0.0\n",
      "c1 a1 0.226\n",
      "c1 a2 0.1668\n",
      "c1 b1 0.0\n",
      "c1 b2 0.0\n",
      "c1 c2 0.0\n",
      "{'a1': 0.3275454354072751, 'a2': 0.23126672821988817, 'b1': 2.478794175090887e-33, 'b2': 0.05926049636325986, 'c2': 1.0458032255925995e-05, 'c1': 0.014085118346995629}\n"
     ]
    }
   ],
   "source": [
    "#Loook at associations and values learning\n",
    "for o1 in associations:\n",
    "    for o2 in associations:\n",
    "        if o1 != o2:\n",
    "            print o1,o2,associations[o1][o2]/nsteps\n",
    "print V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
